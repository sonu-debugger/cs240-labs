{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozw_JJzg1mOQ",
        "outputId": "aa314c81-acfc-4a7e-bc99-4f79eafb05f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download the Brown corpus and the universal tagset\n",
        "nltk.download(\"brown\")\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iQKL1wBz1mOU"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "# Get the tagged sentences from the Brown corpus\n",
        "tagged_sentences = brown.tagged_sents(tagset=\"universal\")\n",
        "tagged_sentences = list(tagged_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "3HwyGueV5kHd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HMMPOSTagger:\n",
        "    def __init__(self, tagged_sentences: List[List[Tuple[str, str]]]):\n",
        "        \"\"\"\n",
        "        Initialize HMM POS Tagger with BOS and EOS tokens\n",
        "\n",
        "        :param tagged_sentences: List of sentences with (word, tag) tuples\n",
        "        \"\"\"\n",
        "        # Add special tokens\n",
        "        self.BOS_TOKEN = '<BOS>'\n",
        "        self.EOS_TOKEN = '<EOS>'\n",
        "\n",
        "        # Extract unique tags and words\n",
        "        # Include BOS and EOS tags\n",
        "        base_tags = sorted(set(tag for sent in tagged_sentences for _, tag in sent))\n",
        "        self.tags = [self.BOS_TOKEN, self.EOS_TOKEN] + base_tags\n",
        "\n",
        "        # Extract words (no special treatment for words)\n",
        "        self.words = sorted(set(word for sent in tagged_sentences for word, _ in sent))\n",
        "\n",
        "        # Create mappings\n",
        "        self.tag_to_index = {tag: i for i, tag in enumerate(self.tags)}\n",
        "        self.word_to_index = {word: i for i, word in enumerate(self.words)}\n",
        "\n",
        "        self.tagged_sentences = tagged_sentences\n",
        "\n",
        "    def train(self, training_sentences: List[List[Tuple[str, str]]], alpha: float = 1.0) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Train HMM model with Add-one smoothing, handling BOS and EOS tokens\n",
        "\n",
        "        :param training_sentences: Training sentences with (word, tag) tuples\n",
        "        :param alpha: Smoothing parameter (default 1.0 for Laplace smoothing)\n",
        "        :return: Transition and emission probability matrices\n",
        "        \"\"\"\n",
        "        n_tags = len(self.tags)\n",
        "        n_words = len(self.words)\n",
        "\n",
        "        # Initialize count matrices with smoothing\n",
        "        transition_counts = np.ones((n_tags, n_tags)) * alpha\n",
        "        emission_counts = np.ones((n_tags, n_words)) * alpha\n",
        "\n",
        "        # Count tag transitions and word emissions\n",
        "        for sent in training_sentences:\n",
        "            # Add BOS and EOS tokens to the sentence\n",
        "            augmented_sent = [\n",
        "                (self.BOS_TOKEN, self.BOS_TOKEN),  # Beginning of sentence\n",
        "                *sent,\n",
        "                (self.EOS_TOKEN, self.EOS_TOKEN)   # End of sentence\n",
        "            ]\n",
        "\n",
        "            # Count tag transitions\n",
        "            for i in range(len(augmented_sent) - 1):\n",
        "                current_tag = augmented_sent[i][1]\n",
        "                next_tag = augmented_sent[i+1][1]\n",
        "\n",
        "                current_idx = self.tag_to_index[current_tag]\n",
        "                next_idx = self.tag_to_index[next_tag]\n",
        "\n",
        "                transition_counts[current_idx, next_idx] += 1\n",
        "\n",
        "            # Count word emissions (excluding BOS and EOS tokens)\n",
        "            for word, tag in sent:\n",
        "                tag_idx = self.tag_to_index[tag]\n",
        "                word_idx = self.word_to_index.get(word, -1)\n",
        "\n",
        "                if word_idx != -1:\n",
        "                    emission_counts[tag_idx, word_idx] += 1\n",
        "\n",
        "        # Normalize to get probabilities\n",
        "        transition_matrix = transition_counts / transition_counts.sum(axis=1, keepdims=True)\n",
        "        emission_matrix = emission_counts / emission_counts.sum(axis=1, keepdims=True)\n",
        "\n",
        "        return transition_matrix, emission_matrix\n",
        "\n",
        "    def cross_validate(self, n_folds: int = 5, alpha: float = 1.0) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Perform n-fold cross-validation\n",
        "\n",
        "        :param n_folds: Number of folds\n",
        "        :param alpha: Smoothing parameter\n",
        "        :return: List of (transition_matrix, emission_matrix) for each fold\n",
        "        \"\"\"\n",
        "        # Prepare k-fold cross-validation\n",
        "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        # Store results for each fold\n",
        "        fold_results = []\n",
        "\n",
        "        # Convert sentences to a flat list\n",
        "        all_sentences = list(self.tagged_sentences)\n",
        "\n",
        "        # Perform cross-validation\n",
        "        for train_index, test_index in kf.split(all_sentences):\n",
        "            # Split into training and test sets\n",
        "            train_sentences = [all_sentences[i] for i in train_index]\n",
        "\n",
        "            # Train the model\n",
        "            transition_matrix, emission_matrix = self.train(train_sentences, alpha)\n",
        "\n",
        "            fold_results.append((transition_matrix, emission_matrix))\n",
        "\n",
        "        return fold_results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gs7QTY2T7j0V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Download Brown Corpus with Universal tagset\n",
        "    tagged_sentences = brown.tagged_sents(tagset=\"universal\")\n",
        "\n",
        "    # Create HMM POS Tagger\n",
        "    hmm_tagger = HMMPOSTagger(tagged_sentences)\n",
        "\n",
        "    # Perform 5-fold cross-validation\n",
        "    fold_results = hmm_tagger.cross_validate()\n",
        "\n",
        "    # Print basic information about the results\n",
        "    print(f\"Number of folds processed: {len(fold_results)}\")\n",
        "\n",
        "    # Print details of the first fold\n",
        "    print(\"\\nFirst Fold Analysis:\")\n",
        "    transition_matrix, emission_matrix = fold_results[0]\n",
        "\n",
        "    print(\"\\nTags (including BOS and EOS):\")\n",
        "    print(hmm_tagger.tags)\n",
        "\n",
        "    print(\"\\nTransition Matrix Shape:\", transition_matrix.shape)\n",
        "    print(\"Emission Matrix Shape:\", emission_matrix.shape)\n",
        "\n",
        "    # Print transition probabilities for BOS and EOS\n",
        "    print(\"\\nBOS Transition Probabilities:\")\n",
        "    bos_idx = hmm_tagger.tag_to_index[hmm_tagger.BOS_TOKEN]\n",
        "    print(transition_matrix[bos_idx, :])\n",
        "\n",
        "    print(\"\\nEOS Transition Probabilities:\")\n",
        "    eos_idx = hmm_tagger.tag_to_index[hmm_tagger.EOS_TOKEN]\n",
        "    print(transition_matrix[eos_idx, :])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCN_QnQkO4ZY",
        "outputId": "8668af16-dfbf-45c3-a278-53e1a743623c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of folds processed: 5\n",
            "\n",
            "First Fold Analysis:\n",
            "\n",
            "Tags (including BOS and EOS):\n",
            "['<BOS>', '<EOS>', '.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n",
            "\n",
            "Transition Matrix Shape: (14, 14)\n",
            "Emission Matrix Shape: (14, 56057)\n",
            "\n",
            "BOS Transition Probabilities:\n",
            "[2.17931395e-05 2.17931395e-05 8.91775269e-02 3.39755045e-02\n",
            " 1.23763239e-01 9.01582182e-02 4.84461492e-02 2.12962559e-01\n",
            " 1.40914440e-01 1.67807174e-02 1.60223162e-01 3.70701303e-02\n",
            " 4.60706969e-02 4.14069651e-04]\n",
            "\n",
            "EOS Transition Probabilities:\n",
            "[0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
            " 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
            " 0.07142857 0.07142857]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPv1wu4jO8Wg"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}